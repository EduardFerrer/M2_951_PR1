{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReferÃ¨ncies\n",
    "## Llibreries emprades\n",
    "- ['sys'](https://docs.python.org/3/library/sys.html): provides access to variables and functions strongly related with the interpreter\n",
    "- ['requests'](https://requests.readthedocs.io/en/latest/): allows you to send HTTP requests using Python.\n",
    "- ['Beautiful Soup'](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): allows you to pull data out of HTML and XML files.\n",
    "- ['re'](https://docs.python.org/3/library/re.html): provides regular expression matching operations similar to those found in Perl.\n",
    "- ['pycodestyle_magic'](https://pypi.org/project/pycodestyle/): provides Python code validation following PEP8 conventions.\n",
    "- ['time'](https://docs.python.org/3/library/time.html#module-time): provides time-related functions.\n",
    "- ['dateutil'](https://dateutil.readthedocs.io/en/stable/): provides powerful extensions to the standard datetime module, available in Python.\n",
    "- ['getuseragent'](https://pypi.org/project/getuseragent/): produces random, commonly used user agents each time you acces a website.\n",
    "- ['PySide6'](https://pypi.org/project/PySide6/): provides access to the complete Qt 6.0+ application development framework.\n",
    "\n",
    "## Altres\n",
    "https://realpython.com/python-download-file-from-url/#using-urllib-from-the-standard-library\n",
    "https://realpython.com/web-scraping-with-scrapy-and-mongodb/#install-the-scrapy-package\n",
    "https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: pycodestyle in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (2.12.1)\n",
      "Requirement already satisfied: flake8 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (7.1.1)\n",
      "Requirement already satisfied: pycodestyle_magic in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (0.5)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from flake8) (0.7.0)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from flake8) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from python-dateutil) (1.16.0)\n",
      "Requirement already satisfied: getuseragent in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pyside6 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (6.8.0.2)\n",
      "Requirement already satisfied: shiboken6==6.8.0.2 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from pyside6) (6.8.0.2)\n",
      "Requirement already satisfied: PySide6-Essentials==6.8.0.2 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from pyside6) (6.8.0.2)\n",
      "Requirement already satisfied: PySide6-Addons==6.8.0.2 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from pyside6) (6.8.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install pycodestyle  flake8 pycodestyle_magic\n",
    "!pip install python-dateutil\n",
    "!pip install getuseragent\n",
    "!pip install pyside6\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from getuseragent import UserAgent\n",
    "from PySide6.QtCore import *\n",
    "from PySide6.QtWidgets import *\n",
    "\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\eferr\\.ai-navigator\\conda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/9.7 MB 7.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/9.7 MB 11.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/9.7 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/9.7 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.7 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.8/9.7 MB 16.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.4/9.7 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.9/9.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 14.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 14.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.5/9.7 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 481.7/481.7 kB 31.4 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.0/63.0 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-24.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:1: E402 module level import not at top of file\n"
     ]
    }
   ],
   "source": [
    "# Dynamic website scraping alternative using Selenium\n",
    "!pip install selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/3.8 MB 812.7 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.5/3.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.7/3.8 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.8/3.8 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 14.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n",
      "Collecting downloader\n",
      "  Downloading downloader-0.98.tar.gz (6.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: downloader\n",
      "  Building wheel for downloader (setup.py): started\n",
      "  Building wheel for downloader (setup.py): finished with status 'done'\n",
      "  Created wheel for downloader: filename=downloader-0.98-py3-none-any.whl size=8154 sha256=b3a7b02727d264a0b4ffe5f25ebe218c2e71d42416058f58fd89923664f722de\n",
      "  Stored in directory: c:\\users\\eferr\\appdata\\local\\pip\\cache\\wheels\\b2\\7a\\ea\\524b97c48a87e019fd6866b7863ab1b62b9c24a395b69cac2d\n",
      "Successfully built downloader\n",
      "Installing collected packages: downloader\n",
      "Successfully installed downloader-0.98\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install downloader\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Downloader\n\u001b[0;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install urlparse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murlparse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eferr\\.ai-navigator\\conda\\Lib\\site-packages\\downloader.py:26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mDownload URLs using a compressed disk cache and a random throttling interval.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mthrottling interval).\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib2\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msqlite3\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mzlib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcStringIO\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:1: E402 module level import not at top of file\n",
      "5:1: E402 module level import not at top of file\n",
      "7:1: E402 module level import not at top of file\n"
     ]
    }
   ],
   "source": [
    "# Exemple google\n",
    "!pip install lxml\n",
    "import lxml.html\n",
    "!pip install downloader\n",
    "from downloader import Downloader\n",
    "!pip install urlparse\n",
    "import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:59: W291 trailing whitespace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15'}\n"
     ]
    }
   ],
   "source": [
    "# Set 'useragent' for the requests to mimic human browsing \n",
    "useragent = UserAgent(requestsPrefix=True).Random()\n",
    "print(useragent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "# Set the target URL (example URL, please replace with the actual target\n",
    "# page)\n",
    "url = 'https://www.idealista.com/en/venta-viviendas/madrid-madrid/'\n",
    "\n",
    "# Set request headers to mimic a browser request\n",
    "headers = useragent\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check response status\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all property listing items\n",
    "    listings = soup.find_all('article', class_='item')\n",
    "\n",
    "    # Extract data for each property\n",
    "    for listing in listings:\n",
    "        # Get the property title\n",
    "        title = listing.find('a', class_='item-link').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Get the property price\n",
    "        price = listing.find('span', class_='item-price').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Get the property location\n",
    "        location = listing.find('span', class_='item-detail').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Print the data\n",
    "        print(f'Title: {title}')\n",
    "        print(f'Price: {price}')\n",
    "        print(f'Location: {location}')\n",
    "        print('-' * 50)\n",
    "else:\n",
    "    print('Failed to retrieve the webpage. Status code:',\n",
    "          response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6:80: E501 line too long (153 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# Set the target URL (example URL, please replace with the actual target\n",
    "# page)\n",
    "url = 'https://www.idealista.com/en/venta-viviendas/madrid-madrid/'\n",
    "\n",
    "# Set request headers to mimic a browser request\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0'}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check response status\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all property listing items\n",
    "    listings = soup.find_all('article', class_='item')\n",
    "\n",
    "    # Extract data for each property\n",
    "    for listing in listings:\n",
    "        # Get the property title\n",
    "        title = listing.find('a', class_='item-link').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Get the property price\n",
    "        price = listing.find('span', class_='item-price').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Get the property location\n",
    "        location = listing.find('span', class_='item-detail').get_text(\n",
    "            strip=True)\n",
    "\n",
    "        # Print the data\n",
    "        print(f'Title: {title}')\n",
    "        print(f'Price: {price}')\n",
    "        print(f'Location: {location}')\n",
    "        print('-' * 50)\n",
    "else:\n",
    "    print('Failed to retrieve the webpage. Status code:',\n",
    "          response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
